<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
		<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
		<title>Hanbyeol Lee | Portfolio</title>

		<link rel="preconnect" href="https://fonts.gstatic.com">
		<link href="https://fonts.googleapis.com/css2?family=Poppins:wght@100;200;300;400;500;600&display=swap" rel="stylesheet">
		<style>
			* { font-family: 'Poppins', sans-serif; }
			body { margin: 0; padding: 0; }
		</style>

		<link href="style.css" rel="stylesheet" type="text/css" />

		<!-- favicon -->
		<link rel="apple-touch-icon" sizes="57x57" href="../assets/favicon.ico/apple-icon-57x57.png">
		<link rel="apple-touch-icon" sizes="60x60" href="../assets/favicon.ico/apple-icon-60x60.png">
		<link rel="apple-touch-icon" sizes="72x72" href="../assets/favicon.ico/apple-icon-72x72.png">
		<link rel="apple-touch-icon" sizes="76x76" href="../assets/favicon.ico/apple-icon-76x76.png">
		<link rel="apple-touch-icon" sizes="114x114" href="../assets/favicon.ico/apple-icon-114x114.png">
		<link rel="apple-touch-icon" sizes="120x120" href="../assets/favicon.ico/apple-icon-120x120.png">
		<link rel="apple-touch-icon" sizes="144x144" href="../assets/favicon.ico/apple-icon-144x144.png">
		<link rel="apple-touch-icon" sizes="152x152" href="../assets/favicon.ico/apple-icon-152x152.png">
		<link rel="apple-touch-icon" sizes="180x180" href="../assets/favicon.ico/apple-icon-180x180.png">
		<link rel="icon" type="image/png" sizes="192x192"  href="../assets/favicon.ico/android-icon-192x192.png">
		<link rel="icon" type="image/png" sizes="32x32" href="../assets/favicon.ico/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="96x96" href="../assets/favicon.ico/favicon-96x96.png">
		<link rel="icon" type="image/png" sizes="16x16" href="../assets/favicon.ico/favicon-16x16.png">

		<style type="text/css">
		a { text-decoration:none }
		a { text-decoration: none; color: black; }
    a:visited { text-decoration: none; }
    a:hover { text-decoration: none; }
    a:focus { text-decoration: none; }
    a:hover, a:active { text-decoration: none; }
		</style>

	</head>
  <body>

    <header class="header">
			<div class="header-container">
				<a class="logo" href="../index.html">
					<h3>Hanbyeol Lee</h3>
				</a>
				<div class="menu">
					<a class="menu-item" href="../index.html">Projects</a>
					<a class="menu-item" href="../about.html">About</a>
				</div>
			</div>
		</header>

    <img src="../assets/image/01_SmartSpeaker/main.jpg">

    <!-- partial:index.partial.html -->
    <main>
    	<div class="main-container">

    		<h1>This or That</h1>
        <h3>소셜 로봇의 포인팅 제스처와 발화 방식에 따른 사용자의 인식 연구</h3>

        <section style="margin-top:2rem">
          <div class="project-info">
            <p>
              <b>Type</b><br>
              <span style="font-size:14px">Human-Robot Interaction 연구</span>
            </p>
            <p>
              <b>Affiliation</b><br>
              <span style="font-size:14px">KIST 지능로봇연구단</span>
            </p>
            <p>
              <b>Role</b><br>
              <span style="font-size:14px">인터랙션 리서처</span><br>
              <span style="font-size:12px; color:gray;">프로젝트 기획, 문헌 조사, 인터랙션 디자인,
                제품 디자인, 프로토타입 개발, 사용자 조사, 데이터 분석, 논문 집필</span>
            </p>
            <p>
              <b>Team</b><br>
              <span style="font-size:14px">Dr. Dahyun Kang (KIST) </span><span style="font-size:12px; color:gray;">프로젝트 기획, 문헌 조사, 실험 설계, 사용자 조사, 데이터 분석, 논문 집필</span><br>
              <span style="font-size:14px">Dr. Eun Ho Kim (KITECH) </span><span style="font-size:12px; color:gray;">프로젝트 기획, 실험 설계, 데이터 분석</span><br>
							<span style="font-size:14px">Dr. Sonya S. Kwak (KIST) </span><span style="font-size:12px; color:gray;">프로젝트 기획, 실험 설계, 사용자 조사, 데이터 분석, 프로젝트 지도</span>
            </p>
            <p>
              <b>Tools</b><br>
							<span style="font-size:14px">
                SolidWorks <span style="font-size:12px; color:gray;">3D모델링</span>,
                Raspberry Pi <span style="font-size:12px; color:gray;">하드웨어</span>,
                TurtleBot <span style="font-size:12px; color:gray;">로봇플랫폼</span>,
                SPSS <span style="font-size:12px; color:gray;">데이터분석</span>
              </span>
            </p>
            <p>
              <b>Output</b><br>
              <span style="font-size:14px">제품 프로토타입, 연구 논문</span>
            </p>
						<p>
              <b>Achievement</b><br>
              <span style="font-size:14px"><a href="https://ieeexplore.ieee.org/document/9341067"><u>IEEE/RSJ IROS 2020 학회 논문</u></a></span>
            </p>
            <p>
              <b>Duration</b><br>
              <span style="font-size:14px;">2019.12 ~ 2020.02 (3개월)</span>
            </p>
          </div>
        </section>

        <section id="background">
          <h2>Background</h2>
          <section>
            <p>
							우리는 같은 공간에 있는 사람과 대화할 때 주변의 사물, 장소 등이 자주 토픽이 되고, 제스처나 지시어를 사용해 위치 정보를 공유한다.
							로봇 또한, 휴대폰이나 스마트 스피커와 달리 언어적 의사소통뿐만 아니라 비언어적 의사소통을 가능하게 하는 물리적인 형태(physical embodiment)를 가지고 있다.
						</p>
						<p>하지만, HRI 측면에서 로봇의 포인팅 연구는 몇 가지 제한이 있었다.</p>
						<div class="quotation">
                <b>Design Question:</b> “사람들과 효과적으로 의사소통할 수 있는 로봇의 포인팅은 어떻게 디자인될 수 있을까?”
            </div>
          </section>
          <section id="locative-deixis">
            <h3>Locative Deixis</h3>
            <p>
							직시어는 대화에서 자주 사용된다. 특히, 위치격 직시어(예시: 이거, 저거)는 주변에 있는 특정 대상을 청자 또는 독자가 이해할 수 있도록 언어를 사용해서 참조할 때 사용된다.
							하지만, 로봇은 언어적 의사소통에 있어 대부분 <b>서술적 직시어</b>(예시: 왼쪽에서 두 번째)를 사용한다.
						</p>
            <div class="img-wrapper">
              <img src="../assets/image/02_ThisorThat/01.png">
              <p style="font-size:12px; color: gray; text-align: center;">직시어는 위치격 직시어와 서술적 직시어로 나뉠 수 있다.</p>
            </div>
          </section>
          <section id="pointing-gesture">
            <h3>Pointing Gesture</h3>
            <p>
							포인팅 제스처는 유아가 첫 단어를 말하기 전 배우는 의사소통 기술 중 하나다<sup id="R01"><a href="#refer01">1</a></sup>.
							사람들은 복잡한 서술을 생략하기 위해 팔, 머리, 눈, 입술, 코와 같은 다양한 신체 부위를 사용하여 포인팅 제스처를 취한다<sup id="R02"><a href="#refer02">2</a></sup>.
							하지만, 대부분의 로봇 포인팅 제스처에 대한 연구는 <b>손가락</b>으로 가리키는 것으로 제한되었다.
            </p>
						<div class="img-wrapper">
              <img src="../assets/image/02_ThisorThat/02.png">
              <p style="font-size:12px; color: gray; text-align: center;">사람은 다양한 신체 부위를 사용하여 포인팅 제스처를 취할 수 있다.</p>
            </div>
          </section>
					<section id="head-pointing">
            <h3>Head Pointing</h3>
            <p>
							사람은 머리를 움직여서 특정 대상을 가리키기도 한다.
							HRI 연구에 따르면 로봇이 머리를 움직여서 가리키는 것은 인간의 행동과 비슷한 효과를 나타낸다<sup id="R03"><a href="#refer03">3</a></sup>.
							사람은 시선과 돌출된 코를 통해 포인팅 제스처를 취할 수 있는 반면에, 소셜 로봇의 머리는 대부분 <b>평면 디스플레이</b>를 사용하기 때문에 정확한 포인팅이 가능한지 알 수 없다.
            </p>
						<div class="img-wrapper">
              <img src="../assets/image/02_ThisorThat/03.png">
              <p style="font-size:12px; color: gray; text-align: center;">사람은 시선과 돌출된 코를 통해 포인팅 제스처를 쉽게 취할 수 있다.</p>
            </div>
          </section>
        </section>

    		<section id="studydesign">
    			<h2>Study Design</h2>
            <section>
              <div class="quotation">
                  <b>Research Question:</b> “로봇의 언어적, 비언어적 포인팅은 사용자의 인식에 어떤 영향을 끼치며, 효과적인 포인팅 인터랙션은 무엇일까?”
              </div>
              <p>로봇의 포인팅 관련 발화 및 제스처를 다양한 조건으로 프로토타이핑한 후, 상황에 따라 로봇의 표현에 대한 사용자의 인식을 정량 조사한다.</p>
            </section>
            <section id="studydesign-iv">
              <h3>Independent Varialbes</h3>
      			  <p>
								로봇은 사용자에게 언어뿐만 아니라 포인팅 제스처를 통해 물체의 위치를 알릴 수 있다.
								또한, 포인팅 제스처는 <b>시선(eye pointing)</b> 확장이다<sup id="R04"><a href="#refer04">4</a></sup>.
								즉, 시선은 포인팅 제스처의 기본이다.
							</p>
							<p>독립 변수는 언어 2가지, 포인팅 제스처 2가지, 시선 2가지로 설정하였다.</p>
              <p>
                <b>언어</b>
                <li>지시적(deictic)</li>
                <li>서술적(descriptive)</li>
              </p>
              <p>
              <b>포인팅 제스처</b>
                <li>코가 있는(with nose)</li>
                <li>코가 없는(without nose)</li>
              </p>
							<p>
              <b>시선</b>
                <li>눈이 있는(with eyes)</li>
                <li>눈이 없는(without eyes)</li>
              </p>
              <div class="img-wrapper">
                <img src="../assets/image/02_ThisorThat/04.png">
                <p style="font-size:12px; color: gray; text-align: center;">독립 변수: 언어(2가지) X 포인팅 제스처(2가지) X 시선(2가지)</p>
              </div>
            </section>
            <section id="studydesign-prototyping">
              <h3>Prototyping</h3>
      			  <p><b>Wizard of Oz</b> 기법 실험을 위한 로봇을 디자인하고 개발하였다.</p>
              <p>
                <b>사용한 소프트웨어</b>
                <li>3D 모델링: SolidWorks</li>
                <li>하드웨어: Raspberry Pi</li>
								<li>로봇플랫폼: TurtleBot</li>
              </p>
              <div class="img-wrapper">
                <img src="../assets/image/02_ThisorThat/05.png">
                <p style="font-size:12px; color: gray; text-align: center;">제품 프로토타입</p>
              </div>
              <details>
                <summary style="cursor:pointer">제품 설계 세부 사항</summary>
                <p>
                  <li>로봇 프로토타입은 ROS 기반 모바일 로봇인 TurtleBot3를 기반으로 방향을 변경하면서 이동할 수 있다.</li>
                  <li>블루투스 통신으로 로봇의 언어 타입을 제어할 수 있다.</li>
                  <li>Raspberry Pi와 연결된 모니터를 통해 로봇의 시선을 제어할 수 있다.</li>
                  <li>로봇의 움직임은 블루투스 통신을 통해 조이스틱 컨트롤러를 통해 수동으로 제어된다.</li>
                </p>
              </details>
            </section>
            <section id="studydesign-participants">
              <h3>Participants</h3>
      			     <p>피험자를 모집하여 연구실 조사(in-lab study)로 실험이 진행되었다.</p>
                 <p>
                   <li>피험자: 48명 (23세~37세 / 여성 26명, 남성 22명)</li>
                 </p>
                 <details>
                   <summary style="cursor:pointer">피험자 설계 세부 사항</summary>
                   <p>
                     <b>혼합 설계 (mixed design)</b>
                   </p>
                   <p>
                     <li>언어: within design - 피험자는 두 가지 언어를 모두 경험한다. (무작위 순서)</li>
                     <li>포인팅 제스처: within design - 피험자는 두 가지 포인팅 제스처를 모두 경험한다. (무작위 순서)</li>
										 <li>시선: between design - 피험자는 한 가지 시선을 경험한다.</li>
                  </p>
               </details>
            </section>
						<section id="studydesign-experiments">
							<h3>Experiments</h3>
								<p>로봇이 사용자와 상호작용하는 상황에 따라, 2가지의 실험을 진행하였다.</p>
								<li>명령형 포인팅 상황: 가리킨 객체에 대한 <b>요청</b> - 자리 안내(seat guide)</li>
								<li>서술형 포인팅 상황: 가리킨 객체에 대한 <b>설명</b> - 전시 안내(exhibition guide)</li>
						</section>
            <section id="studydesign-dv">
              <h3>Measurments</h3>
      			    <p>각 실험 조건에 대해서 아래 4가지를 <b>7점 리커트 척도</b>로 측정하였다.</p>
								<p>
									측정을 통해, 사용자가 각 로봇의 표현 방식에 따라 사용자에게 위치 정보를 얼마나 효과적으로, 사회적으로, 자연스럽게 제공하는지 확인하였다.
									또한, 로봇에 대한 전반적인 인상을 알아보기위해 제품 평가를 조사하였다.
								</p>
                <p>
                  <li>효율성(perceived effectiveness)<sup id="R05"><a href="#refer05">5</a></sup></li>
                  <li>사교성(perceived sociability)<sup id="R06"><a href="#refer06">6</a></sup></li>
                  <li>자연스러움(perceived naturalness)<sup id="R05"><a href="#refer05">5</a></sup></li>
									<li>제품 평가(product evaluation)<sup id="R07"><a href="#refer07">7,</a></sup><sup id="R08"><a href="#refer08"> 8</a></sup></li>
                </p>
								<p>진행되었던 실험 중, 전시 안내 상황에서는 로봇의 설명에 대한 인식을 알아보기 위해 아래 2가지를 추가적으로 측정하였다.</p>
								<p>
									<li>유능함(perceived competency)<sup id="R06"><a href="#refer06">6</a></sup></li>
                  <li>신뢰성(perceived trustworthiness)<sup id="R06"><a href="#refer06">6</a></sup></li>
								</p>
                <details>
                  <summary style="cursor:pointer">질문 아이템</summary>
                  <div class="img-wrapper">
                    <img src="../assets/image/02_ThisorThat/06.png">
										<img src="../assets/image/02_ThisorThat/07.png">
                    <p style="font-size:12px; color: gray; text-align: center;">4가지 공통 측정과 2가지 추가 측정에 대한 아이템</p>
                  </div>
              </details>
            </section>
    		</section>
				<section id="seat-guide">
    			<h2>Seat Guide</h2>
					<p>
						로봇은 피험자를 맞이하고 자리를 안내한다.
						피험자는 연구실에 배치된 5개의 의자 중 로봇이 포인팅 하는 의자에 앉도록 요청받았다.
					</p>
					<div class="img-wrapper">
						<img src="../assets/image/02_ThisorThat/08.png">
						<img src="../assets/image/02_ThisorThat/09.png">
						<p style="font-size:12px; color: gray; text-align: center;">자리 안내 실험 환경</p>
					</div>
					<section id="seat-procedure">
						<h3>Procedure</h3>
						<p>
							피험자는 무작위 순서로 총 4번의 착석 요청을 받았으며, 각 착석 이후에 로봇의 인상을 평가했다.
							로봇의 언어 및 제스처 표현 예는 다음과 같다.
						</p>
						<div class="img-wrapper">
							<img src="../assets/image/02_ThisorThat/10.png">
						</div>
					</section>
					<section id="seat-results">
						<h3>Results</h3>
						<p>실험을 통해 각 조건의 로봇이 사용자의 인식에 미치는 영향을 조사하기 위해 <b>이원 반복측정 분산분석(two-way RM ANOVA)</b>을 수행했다.</p>
						<p>
              <b>사용한 소프트웨어</b>
              <li>데이터 분석: SPSS</li>
            </p>
						<p>로봇의 효율성, 사교성, 자연스러움 및 제품 평가 측정에 대해 신뢰도 분석(Cronbach's alpha) 시, 모두 0.6 이상으로 유의한 결과가 나왔다.</p>
						<details>
              <summary style="cursor:pointer">신뢰도 분석 결과</summary>
                <div class="img-wrapper">
                  <img src="../assets/image/02_ThisorThat/11.png">
                  <p style="font-size:12px; color: gray; text-align: center;">각 측정에 대한 신뢰도 분석 후 Cronbach's alpha 계수</p>
                </div>
            </details>
						<div class="img-wrapper">
              <p style="text-align: center; font-weight:600;">Main Effect</p>
              <p><img src="../assets/image/02_ThisorThat/12.png"></p>
              <p><img src="../assets/image/02_ThisorThat/13.png"></p>
            </div>

						<details>
              <summary style="cursor:pointer; color:#3CB371">측정 항목별 분석</summary>
							<section>
								<h3>Effectiveness</h3>
								<p>
		              포인팅 제스처가 효율성에 미치는 <b>주 효과</b>는 유의미했다.
		              <li>코가 있는 로봇이 코가 없는 로봇보다 더 효율적이라고 인식했다.</li>
		            </p>
		            <details>
		              <summary style="cursor:pointer">주 효과 데이터</summary>
		                <p>
		                  <li><i>F</i><sub>(1,46)</sub> = 21.171, <i>p</i> < 0.0005</li>
		                  <li><i>M</i><sub>withnose</sub> = 5.32, <i>SD</i> = 0.15 vs. <i>M</i><sub>withoutnose</sub> = 4.54, <i>SD</i> = 0.19</li>
		                </p>
		            </details>
								<div class="img-wrapper">
		              <img src="../assets/image/02_ThisorThat/15.png">
		              <p style="font-size:12px; color: gray; text-align: center;">효율성 주 효과: with nose > without nose</p>
		            </div>
							</section>
							<section>
								<h3>Sociability</h3>
								<p>
		              포인팅 제스처가 사교성에 미치는 <b>주 효과</b>는 유의미했다.
		              <li>코가 있는 로봇이 코가 없는 로봇보다 더 사교적이라고 인식했다.</li>
		            </p>
		            <details>
		              <summary style="cursor:pointer">주 효과 데이터</summary>
		                <p>
		                  <li><i>F</i><sub>(1,46)</sub> = 3.870, <i>p</i> = 0.055</li>
		                  <li><i>M</i><sub>withnose</sub> = 4.68, <i>SD</i> = 0.16 vs. <i>M</i><sub>withoutnose</sub> = 4.41, <i>SD</i> = 0.17</li>
		                </p>
		            </details>
								<div class="img-wrapper">
		              <img src="../assets/image/02_ThisorThat/16.png">
		              <p style="font-size:12px; color: gray; text-align: center;">사교성 주 효과: with nose > without nose</p>
		            </div>
							</section>
							<section>
								<h3>Product Evaluation</h3>
								<p>
		              언어와 포인팅 제스처가 제품 평가에 미치는 <b>주 효과</b>는 유의미했다.
									<li>서술적 발화를 하는 로봇이 지시적 발화를 하는 로봇보다 더 긍정적이라고 인식했다.</li>
		              <li>코가 있는 로봇이 코가 없는 로봇보다 더 긍정적이라고 인식했다.</li>
		            </p>
		            <details>
		              <summary style="cursor:pointer">주 효과 데이터</summary>
		                <p>
		                  <li>verbal type: <i>F</i><sub>(1,46)</sub> = 2.955, <i>p</i> = 0.092</li>
		                  <li><i>M</i><sub>deictic</sub> = 4.70, <i>SD</i> = 0.17 vs. <i>M</i><sub>descriptive</sub> = 5.00, <i>SD</i> = 0.15</li>
											<li>pointing gesture: <i>F</i><sub>(1,46)</sub> = 7.736, <i>p</i> = 0.008</li>
		                  <li><i>M</i><sub>withnose</sub> = 5.01, <i>SD</i> = 0.15 vs. <i>M</i><sub>withoutnose</sub> = 4.70, <i>SD</i> = 0.15</li>
		                </p>
		            </details>
								<div class="img-wrapper">
		              <img src="../assets/image/02_ThisorThat/17.png">
		              <p style="font-size:12px; color: gray; text-align: center;">제품 평가 주 효과: descriptive > deictic / with nose > without nose</p>
		            </div>
							</section>
							<section>
								<h3>Accuracy of Location Information Perception</h3>
								<p>위치 정보 인식의 정확성은 다음과 같다. (피험자가 안내 로봇이 의도한 자리에 앉은 확률)</p>
								<div class="img-wrapper">
		              <img src="../assets/image/02_ThisorThat/14.png">
		            </div>
							</section>
						</details>
					</section>
        </section>

				<section id="exhibition-guide">
    			<h2>Exhibition Guide</h2>
						<p>
							피험자는 안내 로봇을 통해 로봇 쇼룸에 전시된 로봇에 대한 설명을 듣는다.
							안내 로봇은 전시된 5개의 로봇을 하나씩 포인팅 하면서 설명하고, 피험자는 메모지에 안내 로봇이 포인팅 한 순서대로 메모한다.
						</p>
						<div class="img-wrapper">
							<img src="../assets/image/02_ThisorThat/18.png">
							<img src="../assets/image/02_ThisorThat/19.png">
							<p style="font-size:12px; color: gray; text-align: center;">전시 안내 실험 환경</p>
						</div>
					<section id="exhibition-procedure">
						<h3>Procedure</h3>
						<p>피험자는 무작위 순서로 총 4번의 조건을 경험했으며, 각 조건 이후에 로봇의 인상을 평가했다. 로봇의 언어 및 제스처 표현 예는 다음과 같다.</p>
						<div class="img-wrapper">
							<img src="../assets/image/02_ThisorThat/20.png">
						</div>
					</section>
					<section id="exhibition-results">
						<h3>Results</h3>
						<p>실험을 통해 각 조건의 로봇이 사용자의 인식에 미치는 영향을 조사하기 위해 <b>이원 반복측정 분산분석(two-way RM ANOVA)</b>을 수행했다.</p>
						<p>
              <b>사용한 소프트웨어</b>
              <li>데이터 분석: SPSS</li>
            </p>
						<p>로봇의 효율성, 사교성, 자연스러움, 유능함, 신뢰성 및 제품 평가 측정에 대해 신뢰도 분석(Cronbach's alpha) 시, 모두 0.6 이상으로 유의한 결과가 나왔다.</p>
						<details>
              <summary style="cursor:pointer">신뢰도 분석 결과</summary>
                <div class="img-wrapper">
                  <img src="../assets/image/02_ThisorThat/21.png">
                  <p style="font-size:12px; color: gray; text-align: center;">각 측정에 대한 신뢰도 분석 후 Cronbach's alpha 계수</p>
                </div>
            </details>
						<div class="img-wrapper">
              <p style="text-align: center; font-weight:600;">Main Effect</p>
              <p><img src="../assets/image/02_ThisorThat/22.png"></p>
              <p><img src="../assets/image/02_ThisorThat/23.png"></p>
							<br>
							<p style="text-align: center; font-weight:600;">Interaction Effect</p>
              <p><img src="../assets/image/02_ThisorThat/24.png"></p>
            </div>
						<details>
              <summary style="cursor:pointer; color:#3CB371">측정 항목별 분석</summary>
							<section>
								<h3>Effectiveness</h3>
								<p>
		              언어와 포인팅 제스처가 효율성에 미치는 <b>주 효과</b>는 유의미했다.
									<li>서술적 발화를 하는 로봇이 지시적 발화를 하는 로봇보다 더 효율적이라고 인식했다.</li>
		              <li>코가 있는 로봇이 코가 없는 로봇보다 더 효율적이라고 인식했다.</li>
		            </p>
		            <details>
		              <summary style="cursor:pointer">주 효과 데이터</summary>
		                <p>
		                  <li>verbal: <i>F</i><sub>(1,46)</sub> = 94.242, <i>p</i> < 0.0005</li>
		                  <li><i>M</i><sub>deictic</sub> = 4.29, <i>SD</i> = 0.17 vs. <i>M</i><sub>descriptive</sub> = 5.63, <i>SD</i> = 0.12</li>
											<li>pointing gesture: <i>F</i><sub>(1,46)</sub> = 53.908, <i>p</i> < 0.0005</li>
		                  <li><i>M</i><sub>withnose</sub> = 5.43, <i>SD</i> = 0.15 vs. <i>M</i><sub>withoutnose</sub> = 4.48, <i>SD</i> = 0.14</li>
		                </p>
		            </details>
								<div class="img-wrapper">
		              <img src="../assets/image/02_ThisorThat/25.png">
		              <p style="font-size:12px; color: gray; text-align: center;">효율성 주 효과: descriptive > deictic / with nose > without nose</p>
		            </div>
							</section>
							<section>
								<h3>Sociability</h3>
								<p>
		              언어와 포인팅 제스처가 사교성에 미치는 <b>주 효과</b>는 유의미했다.
									<li>서술적 발화를 하는 로봇이 지시적 발화를 하는 로봇보다 더 사교적이라고 인식했다.</li>
		              <li>코가 있는 로봇이 코가 없는 로봇보다 더 사교적이라고 인식했다.</li>
		            </p>
		            <details>
		              <summary style="cursor:pointer">주 효과 데이터</summary>
		                <p>
		                  <li>verbal: <i>F</i><sub>(1,46)</sub> = 54.904, <i>p</i> < 0.0005</li>
		                  <li><i>M</i><sub>deictic</sub> = 4.33, <i>SD</i> = 0.13 vs. <i>M</i><sub>descriptive</sub> = 4.99, <i>SD</i> = 0.14</li>
											<li>pointing gesture: <i>F</i><sub>(1,46)</sub> = 8.595, <i>p</i> = 0.005</li>
		                  <li><i>M</i><sub>withnose</sub> = 4.82, <i>SD</i> = 0.14 vs. <i>M</i><sub>withoutnose</sub> = 4.50, <i>SD</i> = 0.13</li>
		                </p>
		            </details>
								<div class="img-wrapper">
		              <img src="../assets/image/02_ThisorThat/26.png">
		              <p style="font-size:12px; color: gray; text-align: center;">사교성 주 효과: descriptive > deictic / with nose > without nose</p>
		            </div>
								<p>
		              또한, 언어와 포인팅 제스처가 사교성에 미치는 <b>상호작용 효과</b>가 존재했다.
		              <li>지시적 발화를 하는 로봇일 때, 코가 있는 로봇이 코가 없는 로봇보다 더 사교적이라고 인식했다.</li>
		            </p>
		            <details>
		              <summary style="cursor:pointer">상호작용 효과 데이터</summary>
		                <p>
		                  <li><i>F</i><sub>(1,46)</sub> = 4.213, <i>p</i> = 0.046</li>
		                  <li><i>M</i><sub>withnose</sub> = 4.58, <i>SD</i> = 1.03 vs. <i>M</i><sub>withoutnose</sub> = 4.08, <i>SD</i> = 1.05, <i>t</i> = 3.500, df = 47, <i>p</i> = 0.001</li>
		                </p>
		            </details>
								<div class="img-wrapper">
		              <img src="../assets/image/02_ThisorThat/27.png">
		              <p style="font-size:12px; color: gray; text-align: center;">사교성 상호작용 효과: [deictic] with nose > without nose</p>
		            </div>
		            <p>로봇에 대해 사용자가 인식하는 <b>사교성</b>을 높이기 위해 <b>지시적 발화(deictic speech)</b> 유형에는 <b>코 포인팅</b>을 적용할 수 있다.</p>
							</section>
							<section>
								<h3>Naturalness</h3>
								<p>
		              언어와 포인팅 제스처가 자연스러움에 미치는 <b>주 효과</b>는 유의미했다.
									<li>서술적 발화를 하는 로봇이 지시적 발화를 하는 로봇보다 더 자연스럽다고 인식했다.</li>
		              <li>코가 있는 로봇이 코가 없는 로봇보다 더 자연스럽다고 인식했다.</li>
		            </p>
		            <details>
		              <summary style="cursor:pointer">주 효과 데이터</summary>
		                <p>
		                  <li>verbal: <i>F</i><sub>(1,46)</sub> = 35.353, <i>p</i> < 0.0005</li>
		                  <li><i>M</i><sub>deictic</sub> = 3.95, <i>SD</i> = 0.15 vs. <i>M</i><sub>descriptive</sub> = 4.73, <i>SD</i> = 0.16</li>
											<li>pointing gesture: <i>F</i><sub>(1,46)</sub> = 35.160, <i>p</i> = 0.005</li>
		                  <li><i>M</i><sub>withnose</sub> = 4.68, <i>SD</i> = 0.16 vs. <i>M</i><sub>withoutnose</sub> = 4.00, <i>SD</i> = 0.15</li>
		                </p>
		            </details>
								<div class="img-wrapper">
		              <img src="../assets/image/02_ThisorThat/28.png">
		              <p style="font-size:12px; color: gray; text-align: center;">자연스러움 주 효과: descriptive > deictic / with nose > without nose</p>
		            </div>
								<p>
		              또한, 포인팅 제스처와 시선이 자연스러움에 미치는 <b>상호작용 효과</b>가 존재했다.
		              <li>눈이 있는 로봇일 때, 코가 있는 로봇이 코가 없는 로봇보다 더 자연스럽다고 인식했다.</li>
									<li>눈이 없는 로봇일 때, 코가 있는 로봇이 코가 없는 로봇보다 더 자연스럽다고 인식했다.</li>
		            </p>
		            <details>
		              <summary style="cursor:pointer">상호작용 효과 데이터</summary>
		                <p>
		                  <li><i>F</i><sub>(1,46)</sub> = 2.914, <i>p</i> = 0.095 (marginally significant)</li>
		                  <li>with eyes: <i>M</i><sub>withnose</sub> = 4.61, <i>SD</i> = 1.35 vs. <i>M</i><sub>withoutnose</sub> = 3.73, <i>SD</i> = 1.41, <i>t</i> = 5.770, df = 47, <i>p</i> = 0.001</li>
		                  <li>without eyes: <i>M</i><sub>withnose</sub> = 4.75, <i>SD</i> = 1.12 vs. <i>M</i><sub>withoutnose</sub> = 4.26, <i>SD</i> = 1.12, <i>t</i> = 3.264, df = 47, <i>p</i> = 0.004</li>
		                </p>
		            </details>
								<div class="img-wrapper">
		              <img src="../assets/image/02_ThisorThat/29.png">
		              <p style="font-size:12px; color: gray; text-align: center;">자연스러움 상호작용 효과: [with eyes] with nose > without nose / [without eyes] with nose > without nose</p>
		            </div>
		            <p>로봇에 대해 사용자가 인식하는 <b>자연스러움</b>을 높이기 위해 로봇의 시선 유무와 상관없이 <b>코 포인팅</b>을 적용할 수 있다.</p>
							</section>
							<section>
								<h3>Competency</h3>
								<p>
		              언어와 포인팅 제스처가 유능함에 미치는 <b>주 효과</b>는 유의미했다.
									<li>서술적 발화를 하는 로봇이 지시적 발화를 하는 로봇보다 더 유능하다고 인식했다.</li>
		              <li>코가 있는 로봇이 코가 없는 로봇보다 더 유능하다고 인식했다.</li>
		            </p>
		            <details>
		              <summary style="cursor:pointer">주 효과 데이터</summary>
		                <p>
		                  <li>verbal: <i>F</i><sub>(1,46)</sub> = 18.574, <i>p</i> < 0.0005</li>
		                  <li><i>M</i><sub>deictic</sub> = 4.37, <i>SD</i> = 0.19 vs. <i>M</i><sub>descriptive</sub> = 4.87, <i>SD</i> = 0.17</li>
											<li>pointing gesture: <i>F</i><sub>(1,46)</sub> = 8.184, <i>p</i> = 0.006</li>
		                  <li><i>M</i><sub>withnose</sub> = 4.76, <i>SD</i> = 0.17 vs. <i>M</i><sub>withoutnose</sub> = 4.48, <i>SD</i> = 0.18</li>
		                </p>
		            </details>
								<div class="img-wrapper">
		              <img src="../assets/image/02_ThisorThat/30.png">
		              <p style="font-size:12px; color: gray; text-align: center;">유능함 주 효과: descriptive > deictic / with nose > without nose</p>
		            </div>
							</section>
							<section>
								<h3>Trustworthiness</h3>
								<p>
		              언어와 포인팅 제스처가 신뢰성에 미치는 <b>주 효과</b>는 유의미했다.
									<li>서술적 발화를 하는 로봇이 지시적 발화를 하는 로봇보다 더 신뢰할 수 있다고 인식했다.</li>
		              <li>코가 있는 로봇이 코가 없는 로봇보다 더 신뢰할 수 있다고 인식했다.</li>
		            </p>
		            <details>
		              <summary style="cursor:pointer">주 효과 데이터</summary>
		                <p>
		                  <li>verbal: <i>F</i><sub>(1,46)</sub> = 43.864, <i>p</i> < 0.0005</li>
		                  <li><i>M</i><sub>deictic</sub> = 4.53, <i>SD</i> = 0.17 vs. <i>M</i><sub>descriptive</sub> = 5.28, <i>SD</i> = 0.15</li>
											<li>pointing gesture: <i>F</i><sub>(1,46)</sub> = 8.799, <i>p</i> = 0.005</li>
		                  <li><i>M</i><sub>withnose</sub> = 5.09, <i>SD</i> = 0.17 vs. <i>M</i><sub>withoutnose</sub> = 4.72, <i>SD</i> = 0.16</li>
		                </p>
		            </details>
								<div class="img-wrapper">
		              <img src="../assets/image/02_ThisorThat/31.png">
		              <p style="font-size:12px; color: gray; text-align: center;">신뢰성 주 효과: descriptive > deictic / with nose > without nose</p>
		            </div>
							</section>
							<section>
								<h3>Product Evaluation</h3>
								<p>
		              언어와 포인팅 제스처가 제품 평가에 미치는 <b>주 효과</b>는 유의미했다.
									<li>서술적 발화를 하는 로봇이 지시적 발화를 하는 로봇보다 더 긍정적으로 인식했다.</li>
		              <li>코가 있는 로봇이 코가 없는 로봇보다 더 긍정적으로 인식했다.</li>
		            </p>
		            <details>
		              <summary style="cursor:pointer">주 효과 데이터</summary>
		                <p>
		                  <li>verbal: <i>F</i><sub>(1,46)</sub> = 56.358, <i>p</i> < 0.0005</li>
		                  <li><i>M</i><sub>deictic</sub> = 4.44, <i>SD</i> = 0.14 vs. <i>M</i><sub>descriptive</sub> = 5.14, <i>SD</i> = 0.12</li>
											<li>pointing gesture: <i>F</i><sub>(1,46)</sub> = 14.507, <i>p</i> < 0.0005</li>
		                  <li><i>M</i><sub>withnose</sub> = 5.00, <i>SD</i> = 0.14 vs. <i>M</i><sub>withoutnose</sub> = 4.59, <i>SD</i> = 0.13</li>
		                </p>
		            </details>
								<div class="img-wrapper">
		              <img src="../assets/image/02_ThisorThat/17.png">
		              <p style="font-size:12px; color: gray; text-align: center;">제품 평가 주 효과: descriptive > deictic / with nose > without nose</p>
		            </div>
								<p>
		              또한, 언어와 시선이 제품 평가에 미치는 <b>상호작용 효과</b>가 존재했다.
		              <li>눈이 있는 로봇일 때, 서술적 발화를 하는 로봇이 지시적 발화를 하는 로봇보다 더 긍정적이라고 인식했다.</li>
									<li>눈이 없는 로봇일 때, 서술적 발화를 하는 로봇이 지시적 발화를 하는 로봇보다 더 긍정적이라고 인식했다.</li>
		            </p>
		            <details>
		              <summary style="cursor:pointer">상호작용 효과 데이터</summary>
		                <p>
		                  <li><i>F</i><sub>(1,46)</sub> = 4.150, <i>p</i> = 0.047</li>
		                  <li>with eyes: <i>M</i><sub>deictic</sub> = 4.31, <i>SD</i> = 1.25 vs. <i>M</i><sub>descriptive</sub> = 5.20, <i>SD</i> = 1.04, <i>t</i> = 5.895, df = 47, <i>p</i> = 0.0005</li>
		                  <li>without eyes: <i>M</i><sub>deictic</sub> = 4.57, <i>SD</i> = 0.96 vs. <i>M</i><sub>descriptive</sub> = 5.08, <i>SD</i> = 0.86, <i>t</i> = 5.002, df = 47, <i>p</i> = 0.0005</li>
		                </p>
		            </details>
								<div class="img-wrapper">
		              <img src="../assets/image/02_ThisorThat/32.png">
		              <p style="font-size:12px; color: gray; text-align: center;">제품 평가 상호작용 효과: [with eyes] descriptive > deictic / [without eyes] descriptive > deictic</p>
		            </div>
		            <p>로봇에 대해 사용자가 인식하는 <b>제품 평가</b>을 높이기 위해 로봇의 시선 유무와 상관없이 <b>서술적 발화(descriptive speech)</b>를 적용할 수 있다.</p>
							</section>
							<section>
								<h3>Accuracy of Location Information Perception</h3>
								<p>
		              언어와 포인팅 제스처, 시선이 위치 정보 인식의 정확성에 미치는 <b>주 효과</b>는 유의미했다.
									<li>지시적 발화를 하는 로봇보다 서술적 발화를 하는 로봇을 통해 정보를 더 정확하게 인식했다.</li>
		              <li>코가 없는 로봇보다 코가 있는 로봇을 통해 정보를 더 정확하게 인식했다.</li>
									<li>눈이 있는 로봇보다 눈이 없는 로봇을 통해 정보를 더 정확하게 인식했다.</li>
		            </p>
		            <details>
		              <summary style="cursor:pointer">주 효과 데이터</summary>
		                <p>
											(10문제 기준)
		                  <li>verbal: <i>F</i><sub>(1,46)</sub> = 8.452, <i>p</i> = 0.006</li>
		                  <li><i>M</i><sub>deictic</sub> = 9.58, <i>SD</i> = 0.11 vs. <i>M</i><sub>descriptive</sub> = 9.91, <i>SD</i> = 0.04</li>
											<li>pointing gesture: <i>F</i><sub>(1,46)</sub> = 4.699, <i>p</i> = 0.035</li>
		                  <li><i>M</i><sub>withnose</sub> = 9.88, <i>SD</i> = 0.06 vs. <i>M</i><sub>withoutnose</sub> = 9.62, <i>SD</i> = 0.11</li>
											<li>gaze: <i>F</i><sub>(1,46)</sub> = 7.614, <i>p</i> = 0.008</li>
		                  <li><i>M</i><sub>witheyes</sub> = 9.58, <i>SD</i> = 0.08 vs. <i>M</i><sub>withouteyes</sub> = 9.91, <i>SD</i> = 0.08</li>
		                </p>
		            </details>
								<div class="img-wrapper">
		              <img src="../assets/image/02_ThisorThat/33.png">
		            </div>
								<p>
		              또한, 언어와 포인팅 제스처가 유용성에 미치는 <b>상호작용 효과</b>가 존재했다.
		              <li>지시적 발화를 하는 로봇일 때, 코가 없는 로봇보다 코가 있는 로봇을 통해 정보를 더 정확하게 인식했다.</li>
		            </p>
		            <details>
		              <summary style="cursor:pointer">상호작용 효과 데이터</summary>
		                <p>
		                  <li><i>F</i><sub>(1,46)</sub> = 5.914, <i>p</i> = 0.019</li>
		                  <li><i>M</i><sub>withnose</sub> = 9.85, <i>SD</i> = 0.62 vs. <i>M</i><sub>withoutnose</sub> = 9.31, <i>SD</i> = 1.45, <i>t</i> = 2.349, df = 47, <i>p</i> = 0.023</li>
		                </p>
		            </details>
		            <div class="img-wrapper">
		              <img src="../assets/image/02_ThisorThat/34.png">
		            </div>
							</section>
						</details>
					</section>
        </section>

        <section id="conclusion">
    			<h2>Conclusion</h2>
					<section id="conclusion-interpretation">
    				<h3>Interpretation of Results</h3>
    				<p>결과에 대한 해석은 다음과 같다.</p>
              <li>사람들은 명령문과 평서문 포인팅 상황에 상관없이 서술적 언어 유형과 코 포인팅을 사용하여 위치 정보를 제공하는 것을 전반적으로 선호하였다.</li>
              <li>위치 정보 인식의 정확성은 언어 유형이 중요하다. 자리 안내 상황에서는 피실험자와 로봇이 마주보고 있었기 때문에 왼쪽과 오른쪽을 반대로 생각하는 사람들이 존재했다.</li>
					</section>
    			<section id="conclusion-implication">
    				<h3>Implication</h3>
    				<p>본 연구는 다음과 같은 영향을 줄 수 있다.</p>
              <li>인간의 사회적 상호작용 연구에 기여할 수 있다.  인간 연구에서는 코를 탈부착할 수 없지만, 인공물인 로봇에서는 코의 영향에 관한 연구가 가능하다.</li>
              <li>로봇의 발화 스타일 디자인 연구에 기여할 수 있다.  서술적 언어 유형에서는 사용자와 기준점을 일치시켜야 하며, 마주봐야 하는 경우에는 지시적 언어 유형으로 제공되어야 한다.</li>
							<li>로봇의 디자인 개선에 기여할 수 있다.  로봇이 제공한 정보가 사용자에게 치명적인 영향을 미치지 않는 경우, 사용자가 더 선호하는 서술적 언어 유형으로 제공하도록 디자인되어야 한다.</li>
							<div class="img-wrapper">
								<img src="../assets/image/02_ThisorThat/35.png">
								<p style="font-size:12px; color: gray; text-align: center;">사용자와 로봇이 기준점이 같은 경우와 다른 경우</p>
							</div>
    			</section>
    			<section id="conclusion-limitation">
    				<h3>Limitation & Futurework</h3>
    				<p>본 연구의 한계점은 다음과 같다.</p>
              <li>본 연구는 단일 환경을 설정했지만, 사용자와 로봇 사이의 거리를 고려하여 보다 다양하고 자연스러운 실험 환경에서 연구를 수행할 수 있다.</li>
              <li>본 연구는 위치 정보 인식의 정확성을 측정했지만, 사용자가 위치를 인식하는데 걸린 시간을 측정해서 각 조건의 유효성을 알아볼 수 있다.</li>
    			</section>
    		</section>

        <section id="publication">
    			<h2>Publication</h2>
          <p><a href="https://ieeexplore.ieee.org/document/9341067"><u><b>This or That: The Effect of Robot’s Deictic Expression on User’s Perception</b></u> <br /> <font size="2em"><u>2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</u></font></a></p>
        </section>

        <div class="references">
          <h3>References</h3>
          <p style="font-size:12px"><a name="refer01">[1] <a href="https://books.google.co.kr/books?id=JlN4AgAAQBAJ&lpg=PP1&ots=pP-gJzkuK8&dq=Pointing%3A%20Where%20Language%2C%20Culture%2C%20and%20Cognition%20Meet&lr&hl=ko&pg=PP1#v=onepage&q=Pointing:%20Where%20Language,%20Culture,%20and%20Cognition%20Meet&f=false"><u><b>Pointing: Where Language, Culture, and Cognition Meet</b></u></a> (2003) by Sotaro Kita <a href="#R01"><b>↩</b></a></p>
          <p style="font-size:12px"><a name="refer02">[2] <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=51D1CBC4966CC0EDAAF34593B83E30F3?doi=10.1.1.462.5492&rep=rep1&type=pdf"><u><b>The Role of Gesture in Communication and Thinking</b></u></a> (1999) by Susan Goldin-Meadow in Trends in Cognitive Sciences <a href="#R02"><b>↩</b></a></p>
          <p style="font-size:12px"><a name="refer03">[3] <a href="https://ieeexplore.ieee.org/document/6247845"><u><b>We Are Not Contortionists: Coupled Adaptive Learning for Head and Body Orientation Estimation in Surveillance Video</b></u></a> (2012) by Cheng Chen and Jean-Marc Odobez in IEEE Conference on Computer Vision and Pattern Recognition <a href="#R03"><b>↩</b></a></p>
          <p style="font-size:12px"><a name="refer04">[4] <a href="https://books.google.co.kr/books?id=jK5nk7iDgSYC&lpg=PT1&dq=Becoming%20Human%3A%20From%20Pointing%20Gestures%20to%20Syntax&lr&hl=ko&pg=PT1#v=onepage&q=Becoming%20Human:%20From%20Pointing%20Gestures%20to%20Syntax&f=false"><u><b>Becoming Human: From Pointing Gestures to Syntax</b></u></a> (2011) by Teresa Bejarano <a href="#R04"><b>↩</b></a></p>
          <p style="font-size:12px"><a name="refer05">[5] <a href="https://ieeexplore.ieee.org/abstract/document/8542607"><u><b>Robot Deictics: How Gesture and Context Shape Referential Communication</b></u></a> (2014) by Allison Sauppé and Bilge Mutlu in ACM/IEEE HRI <a href="#R05"><b>↩</b></a></p>
          <p style="font-size:12px"><a name="refer06">[6] <a href="https://ieeexplore.ieee.org/abstract/document/6483608"><u><b>Rhetorical Robots: Making Robots More Effective Speakers Using Linguistic Cues of Expertise</b></u></a> (2013) by Sean Andrist, Erin Spannan and Bilge Mutlu in ACM/IEEE HRI <a href="#R06"><b>↩</b></a></p>
          <p style="font-size:12px"><a name="refer07">[7] <a href="https://journals.sagepub.com/doi/10.1509/jmkr.44.2.251"><u><b>Adoption of New and Really New Products: The Effects of Self-Regulation Systems and Risk Salience</b></u></a> (2007) by Michal Herzenstein, Steven S. Posavac and J. Joško Brakus in Journal of Marketing Research <a href="#R07"><b>↩</b></a></p>
          <p style="font-size:12px"><a name="refer08">[8] <a href="https://journals.sagepub.com/doi/10.1509/jmkr.46.1.46"><u><b>The Role of Imagination-Focused Visualization on New Product Evaluation</b></u></a> (1989) by Min Zhao, Steve Hoeffler and Darren W. Dahl in Journal of Marketing Research <a href="#R08"><b>↩</b></a></p>
        </div>

    	</div>

    	<nav class="section-nav">
    		<ol>
    			<li><a href="#background">Background</a>
            <ul>
              <li class=""><a href="#locative-deixis">Locative Deixis</a></li>
              <li class=""><a href="#pointing-gesture">Pointing Gesture</a></li>
							<li class=""><a href="#head-pointing">Head Pointing</a></li>
            </ul>
          </li>
    			<li><a href="#studydesign">Study Design</a>
            <ul>
              <li class=""><a href="#studydesign-iv">Independent Varialbes</a></li>
              <li class=""><a href="#studydesign-prototyping">Prototyping</a></li>
              <li class=""><a href="#studydesign-participants">Participants</a></li>
							<li class=""><a href="#studydesign-experiments">Experiments</a></li>
              <li class=""><a href="#studydesign-dv">Measurements</a></li>
            </ul>
          </li>
    			<li><a href="#seat-guide">Seat Guide</a>
    				<ul>
    					<li class=""><a href="#seat-procedure">Prodecure</a></li>
							<li class=""><a href="#seat-results">Results</a></li>
    				</ul>
    			</li>
					<li><a href="#exhibition-guide">Exhibition Guide</a>
    				<ul>
    					<li class=""><a href="#exhibition-procedure">Prodecure</a></li>
							<li class=""><a href="#exhibition-results">Results</a></li>
    				</ul>
    			</li>
          <li><a href="#conclusion">Conclusion</a>
    				<ul>
							<li class=""><a href="#conclusion-interpretation">Interpretation of Results</a></li>
    					<li class=""><a href="#conclusion-implication">Implication</a></li>
    					<li class=""><a href="#conclusion-limitation">Limitation & Future Work</a></li>
    				</ul>
    			</li>
          <li><a href="#publication">Publication</a></li>
    		</ol>
    	</nav>
    </main>

    <!-- partial -->
    <script  src="./script.js"></script>

    <footer class="footer">
      <div class="footer-container">
        <div class="footer-item">
          <h3>Hanbyeol Lee</h3>
          <a href="mailto:yihaanstar@gmail.com">yihaanstar@gmail.com</a>
        </div>
        <div class="footer-item">
          <h3>Info</h3>
          <a title="CV" href="https://drive.google.com/file/d/1LSWyGow6RW6OpuX-SsTMibbC_1EhydMj/view?usp=sharing">CV</a><br>
          <a title="Scholar" href="https://scholar.google.com/citations?user=J2aMC2gAAAAJ&hl=ko">Google Scholar</a><br>
        </div>
        <div class="copyright">
          © Hanbyeol Lee. 2021
        </div>
      </div>
    </footer>

  </body>
</html>
